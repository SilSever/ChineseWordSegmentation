# -*- coding: utf-8 -*-
"""Chinese_Segmentation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OfTJJHLgtFp86u_EsomtpLzV7zhiixCC
"""

"""
==============================================================
==============================================================
I PUT THE COLAB FILE ONLY AS PROOF THAT I WORK ON THE PROJECT.
THERE ARE SOME FUNCTIONS NOT UPDATE YET. 
N.B. THIS IS NOT THE REAL PROJECT BUT ONLY A WORK FILE.
===============================================================
===============================================================

"""

!wget https://www.dropbox.com/s/4lliail841zw84p/icwb2-data.zip
!unzip icwb2-data.zip

"""# **Preprocess.py**"""

from typing import List
import os
import numpy as np

from typing import List, Dict
from keras.preprocessing.sequence import pad_sequences

from gensim.models import KeyedVectors
from tensorflow.keras.utils import to_categorical

def read_dataset(filename: str) -> List[str]:
    """
    :param path Path to the .utf8 dataset
    :return sentences List of sentences in the dataset
    """
    file = []
    with open(filename, mode='r', encoding='utf8') as f:
        for line in f:
            line = line.strip()
            file.append(line)

    return file

def write_file(to_write, filename):

    file = open(filename, 'a', encoding='utf8')
    for i in to_write:
        file.write(i + "\n")

    file.close()

def to_bies_and_remove_spaces(sentences):
    bies_sentence = []
    no_spaces_sentences = []

    for sentence in sentences:
        if len(sentence) != 0:
          sentence = " " + sentence + " "
          tmp_bies = ""
          tmp_no_spaces = ""

          for i in range(len(sentence)):

              if (sentence[i] != " "):
                  if(sentence[i-1] == " " and sentence[i+1] != " "): tmp_bies += "B"
                  if(sentence[i-1] != " " and sentence[i+1] != " "): tmp_bies += "I"
                  if(sentence[i-1] != " " and sentence[i+1] == " "): tmp_bies += "E"
                  if(sentence[i-1] == " " and sentence[i+1] == " "): tmp_bies += "S"

                  tmp_no_spaces += sentence[i]

          bies_sentence.append(tmp_bies)
          no_spaces_sentences.append(tmp_no_spaces)

    return bies_sentence, no_spaces_sentences

def file_generator():
  gen_path = "icwb2-data"
  path_dict = {"training" : "cityu_training_simplify.utf8",
               "gold"     : "cityu_test_gold_simplify.utf8"}
                #"testing"  : "msr_test.utf8"}

  for folder in path_dict:
    files = path_dict[folder]
    sentences = read_dataset(os.path.join(gen_path, folder, files))
    bies_sentence, no_spaces_sentences = to_bies_and_remove_spaces(sentences)

    write_file(bies_sentence, os.path.join(gen_path, folder, files[:-5] + "_label" + files[-5:]))
    write_file(no_spaces_sentences, os.path.join(gen_path, folder, files[:-5] + "_input" + files[-5:]))

#file_generator()
min_t = len(pku_train)
min_d = len(cityu_dev)

def file_reader():
  gen_path = "icwb2-data"
  path_dict = {"training" : "concatenated.utf8",
               "gold"     : "concatenated_test_gold.utf8"}

  for folder in path_dict:    
      files = path_dict[folder]    
      input_ = read_dataset(os.path.join(gen_path, folder, files[:-5] + "_input" + files[-5:]))[]
      label = read_dataset(os.path.join(gen_path, folder, files[:-5] + "_label" + files[-5:]))
    
    yield input_ , label

#(train_x, train_y), (dev_x, dev_y) = file_reader()

msr_train = read_dataset("icwb2-data/training/msr_training_input.utf8")
as_train = read_dataset("icwb2-data/training/as_training_simplify_input.utf8")
pku_train = read_dataset("icwb2-data/training/pku_training_input.utf8")
cityu_train = read_dataset("icwb2-data/training/cityu_training_simplify_input.utf8")

msr_label = read_dataset("icwb2-data/training/msr_training_label.utf8")
as_label = read_dataset("icwb2-data/training/as_training_simplify_label.utf8")
pku_label = read_dataset("icwb2-data/training/pku_training_label.utf8")
cityu_label = read_dataset("icwb2-data/training/cityu_training_simplify_label.utf8")

msr_dev = read_dataset("icwb2-data/gold/msr_test_gold_input.utf8")
as_dev = read_dataset("icwb2-data/gold/as_testing_gold_simplify_input.utf8")
pku_dev = read_dataset("icwb2-data/gold/pku_test_gold_input.utf8")
cityu_dev = read_dataset("icwb2-data/gold/cityu_testing_gold_simplify_input.utf8")

msr_dev_label = read_dataset("icwb2-data/gold/msr_test_gold_label.utf8")
as_dev_label = read_dataset("icwb2-data/gold/as_testing_gold_simplify_label.utf8")
pku_dev_label = read_dataset("icwb2-data/gold/pku_test_gold_label.utf8")
cityu_dev_label = read_dataset("icwb2-data/gold/cityu_testing_gold_simplify_label.utf8")

print("TRAIN")
print(len(msr_train))
print(len(as_train))
print(len(pku_train))
print(len(cityu_train))
print("\nDEV")
print(len(msr_dev))
print(len(as_dev))
print(len(pku_dev))
print(len(cityu_dev))

min_t = len(pku_train)
min_d = len(cityu_dev)
train_x = msr_train[:min_t] + as_train[:min_t] + cityu_train[:min_t] + pku_train
train_y = msr_label[:min_t] + as_label[:min_t] + cityu_label[:min_t] + pku_label

dev_x = msr_dev[:min_d] + as_dev[:min_d] + cityu_dev + pku_dev[:min_d]
dev_y = msr_dev_label[:min_d] + as_dev_label[:min_d] + cityu_dev_label + pku_dev_label[:min_d]

print(len(train_x))
print(len(train_y))

write_file(train_x, "concatenated_input.utf8")
write_file(train_y, "concatenated_label.utf8")

write_file(dev_x, "concatenated_test_gold_input.utf8")
write_file(dev_y, "concatenated_test_gold_label.utf8")

"""# **Process.py**"""

def split_into_grams(sentence: str, ngram: int) -> List[str]:
    """
    :param sentence Sentence as str
    :return bigrams List of bigrams
    """
    bigrams = []
    
    for i in range(len(sentence)):
      bigram = sentence[i:i+ngram]
      
      if i == len(sentence) - 1 and ngram == 2:      
        bigram += '<end>'
        
      bigrams.append(bigram)

    return np.array(bigrams)

def make_vocab(sentences: List[str], ngram: int) -> Dict[str, int]:
    '''
    :param sentences List of sentences to build vocab from
    :return vocab Dictionary from bigram to int
    '''

    bigrams_vec = []
    vocab = {"UNK": 1, "PAD": 0}
    #vocab = {"<PAD>": 0}

    for sentence in sentences:
        splitted = split_into_grams(sentence, ngram)
        
        for elem in splitted:
            if elem not in vocab:
                vocab[elem] = len(vocab)
    
    return vocab

#FOR X
def input_prep(sentences, vocab, ngram):
  input_matrix = []
  
  for sentence in sentences:
    input_vector = []
    
    splitted = split_into_grams(sentence, ngram)
    for elem in splitted:
      if elem not in vocab:
        input_vector.append(vocab['UNK'])
      else:
        input_vector.append(vocab[elem])
    input_matrix.append(np.array(input_vector))
  
#  maxlen = len(max(input_matrix, key=len))
#  x = pad_sequences(input_matrix, maxlen=maxlen, padding='post')
  return np.array(input_matrix)

#FOR Y
def label_prep(sentences):

  classes = {'B': 0, 'I': 1, 'E': 2, 'S': 3}
  label_matrix = []

  for line in sentences:

    label_vector = []
    splitted = split_into_grams(line, 1)
    for elem in splitted:
      label_vector.append(classes[elem])

    label_vector = to_categorical(label_vector, num_classes=len(classes))
    label_matrix.append(np.array(label_vector))

#  max_len = len(max(label_matrix, key=len))
#  y = pad_sequences(label_matrix, maxlen=max_len, padding='post')

  return np.array(label_matrix)

def embedding_prep(sentences, ngram):
  embedding_vec = []

  for sentence in sentences:
    tmp = ""
    for splitted in split_into_grams(sentence, ngram):
      tmp += splitted + " "
    tmp = tmp[:-1]
    
    embedding_vec.append(tmp)
  
  return np.array(embedding_vec)

"""# **Embedding.py**"""

#ngram_vec = embedding_prep(sentence_train, 1)
#write_file(ngram_vec,"icwb2-data/training/msr_unigram_training_input.utf8")

#ngram_vec = embedding_prep(sentence_train, 2)
#write_file(ngram_vec,"icwb2-data/training/msr_bigram_training_input.utf8")

#!wget https://www.dropbox.com/s/so3svjx3780zgp2/merge_sgns_bigram_char300.txt

EMB_UNI_TRAIN = "icwb2-data/training/msr_unigram_training_embedding_32.utf8"
#EMB_UNI_TRAIN = "merge_sgns_bigram_char300.txt"
unigram_embedding = KeyedVectors.load_word2vec_format(EMB_UNI_TRAIN)

#EMB_BIG_TRAIN = "icwb2-data/training/msr_bigram_training_embedding_32.utf8"
EMB_BIG_TRAIN = "icwb2-data/training/msr_bigram_training_embedding_32.utf8"
bigram_embedding = KeyedVectors.load_word2vec_format(EMB_BIG_TRAIN)

print("Unigram embedding shape --> ", unigram_embedding.vectors.shape)
#bigram_embedding = unigram_embedding
print("Bigram embedding shape --> ", bigram_embedding.vectors.shape)

uni_voc_train = make_vocab(train_x, 1)
big_voc_train = make_vocab(train_x, 2)
#uni_voc_train = make_vocab(vocab_train, 1)
#big_voc_train = make_vocab(vocab_train, 2)


print("uni_voc_train --> ",len(uni_voc_train))
print("big_voc_train --> ",len(big_voc_train))

uni_train  = input_prep(train_x, uni_voc_train, 1)
big_train  = input_prep(train_x, big_voc_train, 2)

uni_dev    = input_prep(dev_x, uni_voc_train, 1)
big_dev    = input_prep(dev_x, big_voc_train, 2)

#uni_test   = input_prep(test_x, uni_voc_train, 1)
#big_test   = input_prep(test_x, big_voc_train, 2)


print("train_uni --> ", uni_train.shape)
print("train_big --> ", big_train.shape)
print("dev_uni   --> ", uni_dev.shape)
print("dev_big   --> ", big_dev.shape)
#print("test_uni  --> ", uni_test.shape)
#print("test_big  --> ", big_test.shape)

uni_emb_matrix = unigram_embedding.vectors
big_emb_matrix  = bigram_embedding.vectors

print("unigram_matrix --> ", uni_emb_matrix.shape)
print("bigram_matrix  --> ", big_emb_matrix.shape)

lab_train = label_prep(train_y)

lab_dev = label_prep(dev_y)

#lab_test = label_prep(test_y)

print("lab_train   --> ", lab_train.shape)
print("lab_dev     --> ", lab_dev.shape)
#print("lab_test    --> ", lab_test.shape)

unigram_embedding.vector_size

def num_unk(vocab, matrix):
  count = 0
  num_char = 0
  len_voc = len(vocab)
  for vector in matrix:
    for elem in vector:
      if elem == len_voc:
        count += 1
      num_char += 1
  return count, num_char, count/num_char

print("######## TRAIN ########")
print("unk_train_uni -> ", num_unk(uni_voc_train, uni_train))
print("unk_train_big -> ", num_unk(big_voc_train, big_train))

print("######## DEV ########")
print("unk_dev_uni -> ", num_unk(uni_voc_train, uni_dev))
print("unk_dev_big -> ", num_unk(big_voc_train, big_dev))

"""# **Model.py**"""

import tensorflow as tf

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Dense, concatenate, Dropout, TimeDistributed
from tensorflow.keras.layers import Bidirectional, LSTM
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import SGD, Adam

import matplotlib.pyplot as plt

EMBEDDING_SIZE=32
HIDDEN_SIZE=256
OUTPUT_SIZE=4
BATCH_SIZE = 64
EPOCHS = 12

unigram_input = Input(shape=(None, ))
bigram_input = Input(shape=(None, ))

unigram_emb = Embedding(len(uni_voc_train), 
                        #unigram_embedding.vectors.shape[0],
                        #output_dim=unigram_embedding.vector_size, 
                        output_dim=EMBEDDING_SIZE, 
                        #weights=[uni_emb_matrix], 
                        trainable=True, 
                        mask_zero=True)(unigram_input)

bigram_emb = Embedding(len(big_voc_train),
                       #bigram_embedding.vectors.shape[0], 
                       #output_dim=bigram_embedding.vector_size, 
                       output_dim=EMBEDDING_SIZE, 
                       #weights=[big_emb_matrix], 
                       trainable=True, 
                       mask_zero=True)(bigram_input)

drop_unigram = Dropout(0.4)(unigram_emb)
drop_bigram  = Dropout(0.4)(bigram_emb)

concatenated_emb = concatenate([drop_unigram, drop_bigram])

#drop_concatenated = Dropout(0.2)(concatenated_emb)

#forward_lstm  = LSTM(HIDDEN_SIZE, dropout=0.2, recurrent_dropout=0.2, return_sequences=True, go_backwards=False)(concatenated_emb)
#backward_lstm = LSTM(HIDDEN_SIZE, dropout=0.2, recurrent_dropout=0.2, return_sequences=True, go_backwards=True)(concatenated_emb)

#concatenated_lstm = concatenate([forward_lstm, backward_lstm])

bidirectional = Bidirectional(LSTM(HIDDEN_SIZE, dropout=0.4, recurrent_dropout=0.4, return_sequences=True))(concatenated_emb)

dense = TimeDistributed(Dense(OUTPUT_SIZE, activation='softmax'))(bidirectional)

model = Model(inputs=[unigram_input, bigram_input], outputs=dense)

def batch_generator(x_uni, x_big, y, batch_size):
  while True:
    for start in range(0, len(x_uni), batch_size):
        end = start + batch_size

        batch_x_uni = x_uni[start:end]
        batch_x_big = x_big[start:end]
        batch_y = y[start:end]

        max_ = len(max(batch_x_uni, key=len))

        input_x_uni = pad_sequences(batch_x_uni, maxlen=max_, padding='post')
        input_x_big = pad_sequences(batch_x_big, maxlen=max_, padding='post')
        label_y     = pad_sequences(batch_y    , maxlen=max_, padding='post')

        yield [input_x_uni, input_x_big], label_y

#WITH GPU
#opt = SGD(lr=0.002, momentum=0.95, decay=1e-4, nesterov=True)
opt = Adam(lr=0.003, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.002, amsgrad=False)

model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['acc'])

model.summary()

#cbk = K.callbacks.TensorBoard("logging/keras_model")
print("\nStarting training...")
#model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size,
#          shuffle=True, validation_data=(dev_x, dev_y), callbacks=[cbk]) 
#print("Training complete.\n")

#model.fit([uni_train, big_train-], lab_train, epochs=EPOCHS, validation_data=([uni_dev, big_dev], lab_dev), batch_size=BATCH_SIZE) 
#print("Training complete.\n")

early_stopping = EarlyStopping(monitor='val_loss', patience=3)
#checkpoint = ModelCheckpoint("", monitor='val_acc', save_best_only=True)
callbacks_list = [early_stopping]

H = model.fit_generator(batch_generator(uni_train, big_train, lab_train, BATCH_SIZE),
                   #steps_per_epoch=int(len(uni_train)/BATCH_SIZE),
                   steps_per_epoch=100,
                   epochs=EPOCHS,
                   #callbacks=callbacks_list,
                   validation_data=batch_generator(uni_dev, big_dev, lab_dev, BATCH_SIZE),
                   #validation_steps=int(len(uni_dev)/BATCH_SIZE)
                   validation_steps=100
                   )

plt.style.use("ggplot")
plt.figure()
N = EPOCHS
plt.plot(np.arange(0, N), H.history["acc"], label="train_acc")
plt.plot(np.arange(0, N), H.history["val_acc"], label="val_acc")
plt.title("Accuracy")
plt.xlabel("Epoch #")
plt.ylabel("Accuracy")
plt.legend(loc="upper left")

plt.style.use("ggplot")
plt.figure()
N = EPOCHS
plt.plot(np.arange(0, N), H.history["loss"], label="train_loss")
plt.plot(np.arange(0, N), H.history["val_loss"], label="val_loss")
plt.title("Loss")
plt.xlabel("Epoch #")
plt.ylabel("Loss")
plt.legend(loc="upper left")

# Save the weights
model.save_weights('bilstm_cityu_10_weights.hdf5')



